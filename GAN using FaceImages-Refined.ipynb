{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6d18339",
   "metadata": {},
   "source": [
    "# GAN using Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4ad7b2",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f21db8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-19 20:49:25.067316: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17a9b14b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.15.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1a94165",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62c0c5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aeff726f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "from IPython import display\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "#import cv2\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from keras.preprocessing.image import img_to_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65eb8d03",
   "metadata": {},
   "source": [
    "## Import and Preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1e6894",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "def load_and_preprocess_images(folder, image_size=(128, 128)):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith('.jpg') or filename.endswith('.png'):  # Add more extensions if needed\n",
    "            img_path = os.path.join(folder, filename)\n",
    "            img = Image.open(img_path)\n",
    "            img = img.resize(image_size)  # Resize image\n",
    "            img = img.convert('RGB')\n",
    "            img_array = np.array(img) / 127.5 - 1  # Normalize to [-1, 1]\n",
    "            #img_array   = np.array(img) / 255.0  # Convert to numpy array and normalize\n",
    "            images.append(img_array)\n",
    "            labels.append(filename)  # Replace with actual label extraction logic\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Load and preprocess images\n",
    "image_folder = 'faces'\n",
    "images, labels = load_and_preprocess_images(image_folder)\n",
    "\n",
    "# Split into training and testing sets\n",
    "train_images, test_images, labels_train, labels_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display shape of datasets\n",
    "print(f'Training set shape: {train_images.shape}, {labels_train.shape}')\n",
    "print(f'Testing set shape: {test_images.shape}, {labels_test.shape}')\n",
    "\n",
    "print(train_images.shape)\n",
    "train_images.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fec09d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './faces'\n",
    "SIZE = 128\n",
    "_img = []\n",
    "\n",
    "image_folder = 'faces'\n",
    "files = os.listdir(image_folder)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in tqdm(files):    \n",
    "        if i == 'seed9090.png':\n",
    "            break\n",
    "        else:  \n",
    "            img = cv2.imread(path + '/'+i,1)\n",
    "            # open cv reads images in BGR format so we have to convert it to RGB\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            #resizing image\n",
    "            img = cv2.resize(img, (SIZE, SIZE))\n",
    "            img = (img - 127.5) / 127.5\n",
    "            imh = img.astype(float)\n",
    "            _img.append(img_to_array(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab60845",
   "metadata": {},
   "source": [
    "## Visualize the  image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca0a555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(sqr = 5):\n",
    "    plt.figure(figsize = (10,10))\n",
    "    plt.title(\"Real Images\",fontsize = 35)\n",
    "    for i in range(sqr * sqr):\n",
    "        plt.subplot(sqr,sqr,i+1)\n",
    "        plt.imshow(_img[i]*0.5 + 0.5 )\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "\n",
    "# to plot images\n",
    "plot_images(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bb2147",
   "metadata": {},
   "source": [
    "### Normalize the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad9b9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape(train_images.shape[0], 128, 128, 3).astype('float32')\n",
    "#print(f'train_images:{train_images.shape}')\n",
    "#print(f'train_images:{train_images}')\n",
    "train_images = (train_images - 127.5) / 127.5  \n",
    "print(f'train_images:{train_images}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5d51f8",
   "metadata": {},
   "source": [
    "### Shuffle the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffa507d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 60000\n",
    "BATCH_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c3a583",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images)\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0898ca9a",
   "metadata": {},
   "source": [
    "## Build Generator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02afe7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 100\n",
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(128*128*3, use_bias=False, input_shape=(latent_dim,)))\n",
    "    model.add(layers.Reshape((128,128,3)))\n",
    "    # downsampling\n",
    "    model.add(tf.keras.layers.Conv2D(128,4, strides=1, padding='same',kernel_initializer='he_normal', use_bias=False))\n",
    "    model.add(tf.keras.layers.Conv2D(128,4, strides=2, padding='same',kernel_initializer='he_normal', use_bias=False))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "    model.add(tf.keras.layers.Conv2D(256,4, strides=1, padding='same',kernel_initializer='he_normal', use_bias=False))\n",
    "    model.add(tf.keras.layers.Conv2D(256,4, strides=2, padding='same',kernel_initializer='he_normal', use_bias=False))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "    model.add(tf.keras.layers.Conv2DTranspose(512, 4, strides=1,padding='same',kernel_initializer='he_normal',use_bias=False))\n",
    "    model.add(tf.keras.layers.Conv2D(512,4, strides=2, padding='same',kernel_initializer='he_normal', use_bias=False))\n",
    "    \n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "    #upsampling\n",
    "    model.add(tf.keras.layers.Conv2DTranspose(512, 4, strides=1,padding='same',kernel_initializer='he_normal',use_bias=False))\n",
    "    model.add(tf.keras.layers.Conv2DTranspose(512, 4, strides=2,padding='same',kernel_initializer='he_normal',use_bias=False))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "    model.add(tf.keras.layers.Conv2DTranspose(256, 4, strides=1,padding='same',kernel_initializer='he_normal',use_bias=False))\n",
    "    model.add(tf.keras.layers.Conv2DTranspose(256, 4, strides=2,padding='same',kernel_initializer='he_normal',use_bias=False))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2DTranspose(128, 4, strides=2,padding='same',kernel_initializer='he_normal',use_bias=False))\n",
    "    model.add(tf.keras.layers.Conv2DTranspose(128, 4, strides=1,padding='same',kernel_initializer='he_normal',use_bias=False))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Conv2DTranspose(3,4,strides = 1, padding = 'same',activation = 'tanh'))\n",
    "    \n",
    "    \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b60f7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "generator = make_generator_model()\n",
    "generator.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e501f0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "noise = np.random.normal(-1,1,(1,100))\n",
    "img = generator(noise)\n",
    "plt.imshow(img[0,:,:,0])\n",
    "plt.show()\n",
    "\n",
    "generated_image = generator(noise, training=False)\n",
    "print(f'Generated Image Shape : {generated_image.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f5a950",
   "metadata": {},
   "source": [
    " ## Build Discriminator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2022ff8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Input((SIZE, SIZE, 3)))\n",
    "    model.add(tf.keras.layers.Conv2D(128,4, strides=2, padding='same',kernel_initializer='he_normal', use_bias=False))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "    model.add(tf.keras.layers.Conv2D(128,4, strides=2, padding='same',kernel_initializer='he_normal', use_bias=False))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "    model.add(tf.keras.layers.Conv2D(256,4, strides=2, padding='same',kernel_initializer='he_normal', use_bias=False))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "    model.add(tf.keras.layers.Conv2D(256,4, strides=2, padding='same',kernel_initializer='he_normal', use_bias=False))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "    model.add(tf.keras.layers.Conv2D(512,4, strides=2, padding='same',kernel_initializer='he_normal', use_bias=False))\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(1,activation = 'sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e230964",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "decision = discriminator(generated_image)\n",
    "print (decision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63cc796",
   "metadata": {},
   "source": [
    "## Define Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f6c11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f937b0",
   "metadata": {},
   "source": [
    "### Discriminator Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ad4580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    print (f'Discriminator Total Loss: {total_loss}')\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e239cf",
   "metadata": {},
   "source": [
    "### Generator Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e7c86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef473ad",
   "metadata": {},
   "source": [
    "### Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce35389",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bd0f94",
   "metadata": {},
   "source": [
    "## Save checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7908f857",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b202b051",
   "metadata": {},
   "source": [
    "## Define the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9feefa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "noise_dim = 4000\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "# You will reuse this seed overtime (so it's easier)\n",
    "# to visualize progress in the animated GIF)\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29af2ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice the use of `tf.function`\n",
    "# This annotation causes the function to be \"compiled\".\n",
    "@tf.function\n",
    "def train_steps(images):\n",
    "    noise = np.random.normal(0,1,(BATCH_SIZE,latent_dim))\n",
    "    with tf.GradientTape() as gen_tape , tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise)\n",
    "        fake_output = discriminator(generated_images)\n",
    "        real_output = discriminator(images)\n",
    "        \n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        dis_loss = discriminator_loss(fake_output, real_output)\n",
    "        \n",
    "        \n",
    "    gradient_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)    \n",
    "    gradient_of_discriminator = disc_tape.gradient(dis_loss, discriminator.trainable_variables)\n",
    "    \n",
    "    generator_optimizer.apply_gradients(zip(gradient_of_generator,generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradient_of_discriminator, discriminator.trainable_variables))\n",
    "    \n",
    "    loss = {'gen loss':gen_loss,\n",
    "           'disc loss': dis_loss}\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bc3bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def train(epochs,dataset):\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        print(\"\\nEpoch : {}\".format(epoch + 1))\n",
    "        for images in dataset:\n",
    "            loss = train_steps(images)\n",
    "        print(\" Time:{}\".format(np.round(time.time() - start),2)) \n",
    "        print(\"Generator Loss: {} Discriminator Loss: {}\".format(loss['gen loss'],loss['disc loss']))\n",
    "           \n",
    "\n",
    "\n",
    "    # Generate after the final epoch\n",
    "   # generate_and_save_images(generator, epochs, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca548868",
   "metadata": {},
   "source": [
    "## Generate and save images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccde040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input, save_dir='images'):\n",
    "    predictions = model(test_input, training=False)\n",
    "    plt.figure(figsize=(10, 2))\n",
    "    \n",
    "    print(f'predictions.shape[0]: {predictions.shape[0]}')\n",
    "    plt.figure(figsize=(10, 2))\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(1, 5, i+1)\n",
    "        plt.imshow((predictions[i] * 0.5 + 0.5))\n",
    "        plt.axis('off')\n",
    "\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "        \n",
    "    plt.savefig(f'{save_dir}/image_at_epoch_{epoch:04d}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b47572b",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f05a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 60000\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 15\n",
    "NOISE_DIM = 4000\n",
    "NUM_EXAMPLES_TO_GENERATE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f49347",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'train_dataset: {train_dataset}')\n",
    "print(f'EPOCHS: {EPOCHS}')\n",
    "#train(train_dataset, EPOCHS)\n",
    "\n",
    "train(EPOCHS,train_dataset)\n",
    "\n",
    "#train(train_dataset, EPOCHS, BATCH_SIZE, NOISE_DIM, NUM_EXAMPLES_TO_GENERATE)\n",
    "\n",
    "#checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d313bb82",
   "metadata": {},
   "source": [
    "## Create a GIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8b625a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_generated_images(square = 5, epochs = 14):\n",
    "    \n",
    "    \n",
    "  plt.figure(figsize = (10,10))\n",
    "  for i in range(square * square):\n",
    "    if epochs != 0:    \n",
    "        if(i == square //2):\n",
    "            plt.title(\"Generated Image at Epoch:{}\\n\".format(epochs), fontsize = 32, color = 'black')\n",
    "    plt.subplot(square, square, i+1)\n",
    "    noise = np.random.normal(0,1,(1,latent_dim))\n",
    "    img = generator(noise)\n",
    "    plt.imshow(np.clip((img[0,...]+1)/2, 0, 1))\n",
    "    \n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid()\n",
    "\n",
    "plot_generated_images(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfc7a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a single image using the epoch number\n",
    "def display_image(epoch_no):\n",
    "    return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e710c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb3725f",
   "metadata": {},
   "outputs": [],
   "source": [
    "anim_file = 'dcgan.gif'\n",
    "\n",
    "with imageio.get_writer(anim_file, mode='I') as writer:\n",
    "    filenames = glob.glob('image*.png')\n",
    "    filenames = sorted(filenames)\n",
    "        image = imageio.imread(filename)\n",
    "        writer.append_data(image)\n",
    "    image = imageio.imread(filename)\n",
    "    writer.append_data(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b4ca45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_docs.vis.embed as embed\n",
    "embed.embed_file(anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf58b4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44229be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
