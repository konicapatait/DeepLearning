{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Load libraries\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\n\nfrom keras.optimizers import Adam\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Flatten, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\nimport pickle\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-09T04:30:52.438812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import Data\n\ntrain_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\n#train_df = train_df[0:200]\n#test_df = test_df[0:200]\nprint(train_df.head())\nprint(f'Train Data Shape: {train_df.shape}')\nprint(f'Test Data Shape: {test_df.shape}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8, 6))\nsns.countplot(x='target', data=train_df)\nplt.title('Target Labels')\nplt.xlabel('Target')\nplt.ylabel('Count')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['text_length'] = train_df['text'].apply(lambda x: len(x.split()))\nplt.figure(figsize=(10, 6))\nsns.histplot(train_df['text_length'], bins=30, kde=True)\nplt.title('Distribution of Text Lengths')\nplt.xlabel('Text Length')\nplt.ylabel('Count')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nsns.countplot(y='keyword', data=train_df, hue='target', order=train_df['keyword'].value_counts().iloc[:15].index)\nplt.title('Top 15 Keywords by Target Label')\nplt.xlabel('Count')\nplt.ylabel('Keyword')\nplt.legend(title='Target')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nsns.countplot(y='location', data=train_df, hue='target', order=train_df['location'].value_counts().iloc[:15].index)\nplt.title('Top 15 Locations by Target Label')\nplt.xlabel('Count')\nplt.ylabel('Location')\nplt.legend(title='Target')\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from wordcloud import WordCloud\n\ndisaster_tweets = train_df[train_df['target'] == 1]['text']\nnon_disaster_tweets = train_df[train_df['target'] == 0]['text']\n\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nwordcloud_disaster = WordCloud(width=800, height=400, background_color='white').generate(' '.join(disaster_tweets))\nplt.imshow(wordcloud_disaster, interpolation='bilinear')\nplt.title('Disaster Tweets')\nplt.axis('off')\n\nplt.subplot(1, 2, 2)\nwordcloud_non_disaster = WordCloud(width=800, height=400, background_color='white').generate(' '.join(non_disaster_tweets))\nplt.imshow(wordcloud_non_disaster, interpolation='bilinear')\nplt.title('Non-Disaster Tweets')\nplt.axis('off')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"modes_keyword = train_df[\"keyword\"].mode()[0] # Get Mode of zeroth value\nmodes_location = test_df[\"location\"].mode()[0] # Get Mode of zeroth value\nprint(f'modes_keyword: {modes_keyword}')\nprint(f'modes_location: {modes_location}')\n\ntrain_df[\"keyword\"] = train_df[\"keyword\"].fillna(modes_keyword) # Assign\ntrain_df[\"location\"] = train_df[\"location\"].fillna(modes_location) # Assign\n\ntest_df[\"keyword\"] = test_df[\"keyword\"].fillna(modes_keyword) # Assign\ntest_df[\"location\"] = test_df[\"location\"].fillna(modes_location) # Assign","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_w = 10000\ntokenizer = Tokenizer(num_words = max_w,oov_token = '<OOV>')\ntokenizer.fit_on_texts(train_df[\"text\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, y_test = train_test_split(train_df, test_size = 0.2, random_state = 0)\nprint(f'y_test: {len(y_test)}')\nprint(f'test_df: {len(test_df)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_length = 200\ntrain_seq = tokenizer.texts_to_sequences(X_train[\"text\"])\ntrain_padded = pad_sequences(train_seq, maxlen = max_length, padding = \"post\", truncating = \"post\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_seq = tokenizer.texts_to_sequences(y_test[\"text\"])\ntest_padded = pad_sequences(test_seq, maxlen = max_length, padding = \"post\", truncating = \"post\")\nprint(f'Test Padded; {test_padded}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, mode='max', restore_best_weights=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s_model = Sequential()\ns_model.add(Embedding(input_dim=max_w, output_dim=128))\ns_model.add(LSTM(64, return_sequences=True))\n#s_model.add(LSTM(64, return_sequences=True))\ns_model.add(Dropout(0.2))\ns_model.add(Dense(1, activation='sigmoid'))\ns_model.add(Flatten())  # Flatten layer to 2D\n#s_model.add(Dense(64, activation = 'relu'))\ns_model.add(Dense(1, activation = 'relu'))\ns_model.add(Dropout(0.2))\n#s_model.add(Dense(32, activation = 'relu'))\ns_model.add(Dense(1, activation = 'sigmoid'))\nprint(\"Model Ready!\")\n\n#flatten_layer = Flatten()(conv_output)\n#dense_layer = Dense(12800, activation='relu')#(flatten_layer)\n#s_model.add(dense_layer)\n\n#model = tf.keras.Sequential([\n#  layers.Embedding(max_features, embedding_dim),\n#  layers.Dropout(0.2),\n#  layers.GlobalAveragePooling1D(),\n#  layers.Dropout(0.2),\n#  layers.Dense(1, activation='sigmoid')])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s_model.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s_model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"y_test['target']: \\n{y_test['target']}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nhistory = s_model.fit(\n    train_padded, \n    X_train['target'],\n    epochs = 10, \n    batch_size=256, \n    validation_data=(test_padded, y_test['target']), \n    callbacks=[early_stopping]\n)\n\n#s_model.fit(train_padded, epochs=1, validation_data=test_generator)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(6,5))\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\n#plt.ylim(0.8, 1)\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_seq = tokenizer.texts_to_sequences(test_df[\"text\"])\ntest_padded = pad_sequences(test_seq, maxlen=max_length, padding=\"post\", truncating=\"post\")\n\n\npredictions = s_model.predict(test_padded)\nprint(f'Predictions: {predictions}')\nbinary_predictions = (predictions > 0.5).astype(int)\n\ntest_df['target'] = binary_predictions\nprint(test_df[['id', 'text', 'target']])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict on validation data\ny_pred = s_model.predict(test_padded)\nprint(y_pred)\ny_pred = (y_pred > 0.5).astype(int)\n\n# Calculate F1-score\nf1 = f1_score(test_df['target'], y_pred)\nprint(f\"F1 Score: {f1}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_flattened = y_pred.flatten()\noutput = pd.DataFrame(\n    {\n    'id': test_df.id, \n    'target': y_pred_flattened\n    }\n)\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test_df.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sample_submission[\"target\"] = np.argmax(s_model.predict(X_test), axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}